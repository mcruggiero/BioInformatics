{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.6.10 |Anaconda, Inc.| (default, Mar 25 2020, 23:51:54) \\n[GCC 7.3.0]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dgorgon_reference.fa\n",
      "harrington_clinical_data.txt\n",
      "hawkins_pooled_sequences.fastq\n",
      "necessary_scripts\n",
      "Week6_Lay_that_Pipe.ipynb\n",
      "\n",
      "faker.sorted.bam\n",
      "faker.sorted.bam.bai\n",
      "getMutations.py\n",
      "parseFastq.py\n",
      "\n",
      "----------------------\n",
      "Line Count of getMutations\n",
      "29 getMutations.py\n",
      "\n",
      "----------------------\n",
      "Line Count of parseFastq.py\n",
      "92 parseFastq.py\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "ls\n",
    "echo \"\"\n",
    "cd necessary_scripts/\n",
    "ls\n",
    "echo \"\"\n",
    "echo \"----------------------\"\n",
    "echo \"Line Count of getMutations\"\n",
    "wc -l getMutations.py\n",
    "echo \"\"\n",
    "echo \"----------------------\"\n",
    "echo \"Line Count of parseFastq.py\"\n",
    "wc -l parseFastq.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files are just too big, to look cat them out above, let's bring these files into jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hawking_pooled_sequences.fastq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@seq13534-419\n",
      "GCAGTAGCGGTCATAAGTGGTACATTACGAGATTCGGAGTACCATAGATTCGCATGAATCCCTGTGGATACGAGAGTGTGAGATATATGTACGCCAATCCAGTGTGATACCCATGAGATTTAGGACCGATGATGGTTGAGGACCAAGGATTGACCCGATGGATGCAGATTTGACCCCAGATAGAATAAATGCGATGAGATGATTTGGCCGATAGATAGATAGTGTCGTGAGGTGACGTCCGTCACTGGACGAA\n",
      "+\n",
      "IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIDIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIFFFFDFFDFFDDFDFDFFFFDDFFDDFDDFF\n",
      "@seq86249-867\n",
      "GGATTAGCGGTCATAAGTCGTACATTACGAGATTCGGAGTACCATAGATTCGCATGAATCCCTGTGGATACGAGAGTGTGAGATATATGTACGCCAATCCAGTGTGATACCCATGAGATTTAGGACCGATGATGGTTGAGGACCAAGGATTGACCCGATGGATGCAGATTTGACCCCAGATAGAATAAATGCGATGAGATGATTTGGCCGATAGATAGATAGAGGTCAGTATAACCTCTCAAAGCTTTATCTACGGATGGATCCGCGC\n",
      "+\n",
      "IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIDIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIDDFDDDDDDFFDFDDFDDDFDFFDDFFFFFFFFFDDFDFFDDFDDF\n",
      "@seq46647-928\n",
      "GACCTAGCGGTCATAAGTGGTACATTACGAGATTCGGAGTACCATAGATTCGCATGAATCCCTGTGGATACGAGAGTGTGAGATATATGTACGCCAATCCAGTGTGATACCCATGAGATTTAGGACCGATGATGGTTGACGACCAAGGATTGACCCGATGGATGCAGATTTGACCCCAGATAGAATAAATGCGATGAGATGATTTGGCCGATAGATAGATAGTAAGTAAATGCCACGGACTCGTCACGTG\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "head -10 hawkins_pooled_sequences.fastq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name\tColor\tBarcode\n",
      "Abbey\tBlack\tGCAGA\n",
      "Adelaide\tOrange\tCTTGA\n",
      "Alana\tBlack\tAGCTA\n",
      "Arabella\tBlack\tTTGAG\n",
      "Arianna\tOrange\tTCCGC\n",
      "Bobby\tBlack\tCCCAC\n",
      "Brayden\tBlack\tGCAGT\n",
      "Bridget\tOrange\tCGGTT\n",
      "Bruce\tOrange\tACGGA\n",
      "Byron\tOrange\tAGCGA\n",
      "Cali\tOrange\tTTTTT\n",
      "Camilla\tOrange\tACAGG\n",
      "Case\tBlack\tGGGAT\n",
      "Cason\tBlack\tAACTG\n",
      "Damien\tBlack\tTGCAA\n",
      "Dania\tBlack\tCATCC\n",
      "Dereon\tOrange\tACACT\n",
      "Destiney\tOrange\tCCGTG\n",
      "Dominik\tBlack\tGGATT\n",
      "Gabrielle\tOrange\tCGAAG\n",
      "Giancarlo\tOrange\tTATGG\n",
      "Greta\tBlack\tCCAAC\n",
      "Israel\tOrange\tCCTTT\n",
      "Javion\tOrange\tTAGCT\n",
      "Jaylen\tBlack\tTCCGG\n",
      "Jazlyn\tYellow\tAGTGA\n",
      "Jordyn\tBlack\tCCACC\n",
      "Jordyna\tYellow\tCATAT\n",
      "Joshua\tBlack\tTACGT\n",
      "Justus\tBlack\tGTTCA\n",
      "Kamden\tOrange\tCCGTA\n",
      "Karlee\tYellow\tGTCGT\n",
      "Kayla\tYellow\tCATAA\n",
      "Keaton\tGreen\tGCTAA\n",
      "Kian\tGreen\tGGAAG\n",
      "Kirsten\tBlack\tTGTGA\n",
      "Libby\tYellow\tAGTCA\n",
      "Lorelei\tOrange\tGCGGA\n",
      "Mariela\tYellow\tCGGAG\n",
      "Martin\tGreen\tCAATT\n",
      "Mckayla\tGreen\tAGAGT\n",
      "Mckenzie\tBlack\tGAAAA\n",
      "Natalia\tGreen\tGATTC\n",
      "Santiago\tOrange\tTGTAT\n",
      "Sarai\tBlack\tCTAGT\n",
      "Serena\tGreen\tGGTAA\n",
      "Shelby\tYellow\tAAATG\n",
      "Sienna\tGreen\tGACCT\n",
      "Terry\tOrange\tAACCC\n",
      "Tristian\tBlack\tTGACG\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cat harrington_clinical_data.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getMutations.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pysam needs python 3.6 downgrading from 3.8 now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing getMutations.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile getMutations.py\n",
    "import pysam\n",
    "\n",
    "#This is a slightly modified version from here: https://pysam.readthedocs.io/en/latest/api.html\n",
    "# What is a pileup? It is \"piling up\" all the reads that have mapped to a given position of your reference.\n",
    "# It is a useful way to see what reads have a mutation and what don't. \n",
    "\n",
    "def pileup():\n",
    "    #test file, replaced with the sorted.bam you are using. Make sure it is indexed! \n",
    "    #(Use samtools index yourbam.sorted.bam)\n",
    "    samfile = pysam.AlignmentFile(\"/home/rbif/week6/necessary_scripts/faker.sorted.bam\", \"rb\")\n",
    "\n",
    "    #Since our reference only has a single sequence, we're going to pile up ALL of the reads. \n",
    "    #Usually you would do it in a specific region (such as chromosome 1, position 1023 to 1050 for example)\n",
    "    for pileupcolumn in samfile.pileup():\n",
    "        print (\"coverage at base %s = %s\" % (pileupcolumn.pos, pileupcolumn.n))\n",
    "        #use a dictionary to count up the bases at each position\n",
    "        ntdict = {}\n",
    "        for pileupread in pileupcolumn.pileups:\n",
    "            if not pileupread.is_del and not pileupread.is_refskip:\n",
    "                # You can uncomment the below line to see what is happening in the pileup. \n",
    "                # print ('\\tbase in read %s = %s' % (pileupread.alignment.query_name, pileupread.alignment.query_sequence[pileupread.query_position]))\n",
    "                base = pileupread.alignment.query_sequence[pileupread.query_position]\n",
    "                \n",
    "                ########## ADD ADDITIONAL CODE HERE ############# \n",
    "                # Populate the ntdict with the counts of each base \n",
    "                # This dictionary will hold all of the base read counts per nucletoide per position.\n",
    "                # Use the dictionary to calculate the frequency of each site, and report it if if the frequency is NOT  100% / 0%. \n",
    "                #############################################\n",
    "        print (ntdict)\n",
    "    samfile.close()\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    pileup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parseFastq.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting parseFastq.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile parseFastq.py\n",
    "\n",
    "import argparse\n",
    "#Example use is \n",
    "# python parseFastq.py --fastq /home/rbif/week6/hawkins_pooled_sequences.fastq\n",
    "\n",
    "\n",
    "################################################\n",
    "# You can use this code and put it in your own script\n",
    "class ParseFastQ(object):\n",
    "    \"\"\"Returns a read-by-read fastQ parser analogous to file.readline()\"\"\"\n",
    "    def __init__(self,filePath,headerSymbols=['@','+']):\n",
    "        \"\"\"Returns a read-by-read fastQ parser analogous to file.readline().\n",
    "        Exmpl: parser.next()\n",
    "        -OR-\n",
    "        Its an iterator so you can do:\n",
    "        for rec in parser:\n",
    "            ... do something with rec ...\n",
    " \n",
    "        rec is tuple: (seqHeader,seqStr,qualHeader,qualStr)\n",
    "        \"\"\"\n",
    "        if filePath.endswith('.gz'):\n",
    "            self._file = gzip.open(filePath)\n",
    "        else:\n",
    "            self._file = open(filePath, 'rU')\n",
    "        self._currentLineNumber = 0\n",
    "        self._hdSyms = headerSymbols\n",
    "         \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "     \n",
    "    def __next__(self):\n",
    "        \"\"\"Reads in next element, parses, and does minimal verification.\n",
    "        Returns: tuple: (seqHeader,seqStr,qualHeader,qualStr)\"\"\"\n",
    "        # ++++ Get Next Four Lines ++++\n",
    "        elemList = []\n",
    "        for i in range(4):\n",
    "            line = self._file.readline()\n",
    "            self._currentLineNumber += 1 ## increment file position\n",
    "            if line:\n",
    "                elemList.append(line.strip('\\n'))\n",
    "            else: \n",
    "                elemList.append(None)\n",
    "         \n",
    "        # ++++ Check Lines For Expected Form ++++\n",
    "        trues = [bool(x) for x in elemList].count(True)\n",
    "        nones = elemList.count(None)\n",
    "        # -- Check for acceptable end of file --\n",
    "        if nones == 4:\n",
    "            raise StopIteration\n",
    "        # -- Make sure we got 4 full lines of data --\n",
    "        assert trues == 4,\\\n",
    "               \"** ERROR: It looks like I encountered a premature EOF or empty line.\\n\\\n",
    "               Please check FastQ file near line number %s (plus or minus ~4 lines) and try again**\" % (self._currentLineNumber)\n",
    "        # -- Make sure we are in the correct \"register\" --\n",
    "        assert elemList[0].startswith(self._hdSyms[0]),\\\n",
    "               \"** ERROR: The 1st line in fastq element does not start with '%s'.\\n\\\n",
    "               Please check FastQ file near line number %s (plus or minus ~4 lines) and try again**\" % (self._hdSyms[0],self._currentLineNumber) \n",
    "        assert elemList[2].startswith(self._hdSyms[1]),\\\n",
    "               \"** ERROR: The 3rd line in fastq element does not start with '%s'.\\n\\\n",
    "               Please check FastQ file near line number %s (plus or minus ~4 lines) and try again**\" % (self._hdSyms[1],self._currentLineNumber) \n",
    "        # -- Make sure the seq line and qual line have equal lengths --\n",
    "        assert len(elemList[1]) == len(elemList[3]), \"** ERROR: The length of Sequence data and Quality data of the last record aren't equal.\\n\\\n",
    "               Please check FastQ file near line number %s (plus or minus ~4 lines) and try again**\" % (self._currentLineNumber) \n",
    "         \n",
    "        # ++++ Return fatsQ data as tuple ++++\n",
    "        return tuple(elemList)\n",
    "##########################################################################\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-f\", \"--fastq\", required=True, help=\"Place fastq inside here\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    #This is an example of how to use the function in your own code\n",
    "    \n",
    "    fastqfile = ParseFastQ(args.fastq)\n",
    "\n",
    "    #A fastq read contains 4 lines\n",
    "    for fastq_obj in fastqfile:\n",
    "        #This fastq_obj is a tuple that has length of 4 and corresponds to those 4 lines\n",
    "        #This is the header\n",
    "        print(fastq_obj[0])\n",
    "        #This is the sequence\n",
    "        print(fastq_obj[1])\n",
    "        #This is the separator\n",
    "        print(fastq_obj[2])\n",
    "        #This is the quality score\n",
    "        print(fastq_obj[3])\n",
    "        \n",
    "        #Just an indicator showing the fastq \"blocks\"\n",
    "        print('*'*10 + '==='*10 + '*' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "python parseFastq.py --fastq /home/rbif/week6/hawkins_pooled_sequences.fastq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important note: We will not be able to use anything from BioPython for this assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demultiplex the pooled fastq by barcode\n",
    "\n",
    "Create a single python script that:\n",
    "1. Reads in the pooled fastq*\n",
    "2. Uses the barcode from clinical data to find matches\n",
    "3. Write to file\n",
    "4. Trims barcode from start\n",
    "5. Trims consequtive D or F from quality scores.\n",
    "6. Set in foder with {name}_trimmed.fastq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello!\n"
     ]
    }
   ],
   "source": [
    "#%%writefile pipeline.py\n",
    "\n",
    "\n",
    "class Main:\n",
    "    def __init__(self):\n",
    "        print(\"hello!\")\n",
    "        \n",
    "    def trimmer(self)\n",
    "    \n",
    "if __name__== \"__main__\":\n",
    "    Main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
