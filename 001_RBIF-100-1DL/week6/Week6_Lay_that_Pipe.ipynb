{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs (found inside /home/rbif/week6/):\n",
    "dgorgon_reference.fa - The wildtype reference sequence of D.gorgon\n",
    "harrington_clinical_data.txt - The clinical data of the samples including a unique patient name, the color of the mold, and a sequence barcode*.\n",
    "hawkins_pooled_sequences.fastq - The fastq data that contains the sequences of all the samples pooled together.\n",
    "*The barcode is the first 5 basepairs of a sequence. It is a unique identifier that determines which sample that read belongs to. So if Tim's barcode was ATCCC then every sequence that begins with ATCCC is a read assigned to Tim.\n",
    "\n",
    "There is a visual representation attached: Visual_pipeline_representation.pdf\n",
    "Steps. Anything highlighted in yellow means I will provide you a python function to use and incorporate into your script. \n",
    "You will not be able to use functions from BioPython for this assignment.\n",
    "1. Demultiplex the pooled fastq - Create a single python script that reads in the pooled fastq* and outputs 50 new fastq files that contain only sequences belonging to that sample. This means using the barcode from the clinical data, finding the match at the start of a given read in the pooled fastq, and writing the read to its respective file. It should at the same time perform the steps 1A and 1B below. \n",
    "*script is /home/rbif/week6/necessary_scripts/parseFastq.py You just need to copy the (class) function into your own script and use the function to help you trim.*\n",
    "1A. Trim the beginning of the reads - Write a python function that removes the barcode and it's associated quality scores from the read.\n",
    "1B. Trim the ends of the reads - Write a python function that removes the ends of the reads based on the following condition: The tail ends of the reads have degraded in quality and must be trimmed before being mapped. If the end of the read has at least two consecutive quality scores of D or F, then remove the rest of the read.\n",
    "\n",
    "\n",
    "Example:\n",
    "From\n",
    "@arbitrary-seq-111\n",
    "ATCGATGCACCCC\n",
    "+\n",
    "IIIDIIIIIDDFF \n",
    "To\n",
    "\n",
    "\n",
    "@arbitrary-seq-111\n",
    "ATCGATGCA\n",
    "+\n",
    "IIIDIIIII\n",
    "**This is any combination of D and/or F**\n",
    "Everything in red would be trimmed off\n",
    "IIIIIFDFF\n",
    "Or\n",
    "IIFFDII\n",
    "Or\n",
    "IIFFIIII\n",
    "\n",
    "Final output of Step 1: 50 fastq files in a folder called fastqs. Each fastq inside of that directory should have the following format:\n",
    "{name}_trimmed.fastq (ie tim_trimmed.fastq)\n",
    "\n",
    "2. Perform alignment on each FASTQ to the reference sequence. Write a python function that calls the bwa command. This will transform the FASTQs to samfiles.\n",
    "*Hint* Use the python packages os or subprocess to call a command line tool. \n",
    "*Note* In order to use bwa, the reference file must first be indexed. This can be accomplished with bwa index ref.fa\n",
    "Then you can use bwa mem ref.fa {name}_trimmed.fastq > {name}.sam\n",
    "to perform the alignment. This is taken from http://bio-bwa.sourceforge.net/bwa.shtml\n",
    "The mapping_to_reference.pdf shows reads that are aligning to the reference. Bwa tries to find the best match for each individual read and maps it to a known reference.\n",
    "3. Convert the samfiles to bamfiles - Use python to wrap the below commands:\n",
    "Use samtools view to convert the SAM file into a BAM file. BAM is the binary format corresponding to the SAM text format. Run:\n",
    "samtools view -bS {name}.sam > {name}.bam\n",
    "Use samtools sort to convert the BAM file to a sorted BAM file.\n",
    "samtools sort {name}.bam {name}.sorted\n",
    "samtools sort -o {name}.sorted.bam {name}.bam\n",
    "\n",
    "Then we need to index the sorted bam.\n",
    "samtools index {name}.sorted.bam \n",
    "We now have a sorted BAM file called {name}.sorted.bam. Sorted BAM is a useful format because the alignments are (a) compressed, which is convenient for long-term storage, and (b) sorted, which is convenient for variant discovery. \n",
    "Taken from: http://bowtie-bio.sourceforge.net/bowtie2/manual.shtml#getting-started-with-bowtie-2-lambda-phage-example\n",
    "Once the sorted.bam files are created, remove the prior .sam files and the .bam files. \n",
    "4. Variant Discovery - Use the python pysam pileup function to discover variants in each sorted bam file. \n",
    "The script is located /home/rbif/week6/necessary_scripts/getMutations.py\n",
    "Copy the function, pileup, to your own script. This needs to be modified. Details on where to modify it are inside the script. \n",
    "You are using this script to find what position along the reference has a mutation. \n",
    "In this case we are looking for a SNP (single nucleotide polymorphism). If a position contains DNA basepairs other than the wildtype, then we can consider it as a SNP and should be reported.\n",
    "**You can consider the reference strain to be wildtype **\n",
    "\n",
    "5. Create a report - Using python, create a report that outputs what nucleotide position and mutation is responsible for each color of the mold. Also print out the number of sequences that were used for each sample. \n",
    "Example:\n",
    "The green mold was caused by a mutation in position 23. The wildtype base was A and the mutation was C.\n",
    "The black moldâ€¦\n",
    "\n",
    "Sample Tim had a green mold, 320 reads, and had 32% of the reads at position 23 had the mutation C. \n",
    "Sample Kristen ...\n",
    "\n",
    "Final Deliverables.\n",
    "A single python script with comments that performs all of the above steps called pipeline.py\n",
    "A folder called fastqs that contains the demultiplexed fastqs for each sample.\n",
    "A folder called bams that contains the sorted.bam file for each sample.\n",
    "A text file called report.txt that looks like step 5.\n",
    "A readme that tells the user how to use the script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demultiplex the pooled fastq by barcode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.6.10 |Anaconda, Inc.| (default, Mar 25 2020, 23:51:54) \\n[GCC 7.3.0]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dgorgon_reference.fa\n",
      "harrington_clinical_data.txt\n",
      "hawkins_pooled_sequences.fastq\n",
      "necessary_scripts\n",
      "Week6_Lay_that_Pipe.ipynb\n",
      "\n",
      "faker.sorted.bam\n",
      "faker.sorted.bam.bai\n",
      "getMutations.py\n",
      "parseFastq.py\n",
      "\n",
      "----------------------\n",
      "Line Count of getMutations\n",
      "29 getMutations.py\n",
      "\n",
      "----------------------\n",
      "Line Count of parseFastq.py\n",
      "92 parseFastq.py\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "ls\n",
    "echo \"\"\n",
    "cd necessary_scripts/\n",
    "ls\n",
    "echo \"\"\n",
    "echo \"----------------------\"\n",
    "echo \"Line Count of getMutations\"\n",
    "wc -l getMutations.py\n",
    "echo \"\"\n",
    "echo \"----------------------\"\n",
    "echo \"Line Count of parseFastq.py\"\n",
    "wc -l parseFastq.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files are just too big, to look cat them out above, let's bring these files into jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hawking_pooled_sequences.fastq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@seq13534-419\n",
      "GCAGTAGCGGTCATAAGTGGTACATTACGAGATTCGGAGTACCATAGATTCGCATGAATCCCTGTGGATACGAGAGTGTGAGATATATGTACGCCAATCCAGTGTGATACCCATGAGATTTAGGACCGATGATGGTTGAGGACCAAGGATTGACCCGATGGATGCAGATTTGACCCCAGATAGAATAAATGCGATGAGATGATTTGGCCGATAGATAGATAGTGTCGTGAGGTGACGTCCGTCACTGGACGAA\n",
      "+\n",
      "IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIDIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIFFFFDFFDFFDDFDFDFFFFDDFFDDFDDFF\n",
      "@seq86249-867\n",
      "GGATTAGCGGTCATAAGTCGTACATTACGAGATTCGGAGTACCATAGATTCGCATGAATCCCTGTGGATACGAGAGTGTGAGATATATGTACGCCAATCCAGTGTGATACCCATGAGATTTAGGACCGATGATGGTTGAGGACCAAGGATTGACCCGATGGATGCAGATTTGACCCCAGATAGAATAAATGCGATGAGATGATTTGGCCGATAGATAGATAGAGGTCAGTATAACCTCTCAAAGCTTTATCTACGGATGGATCCGCGC\n",
      "+\n",
      "IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIDIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIDDFDDDDDDFFDFDDFDDDFDFFDDFFFFFFFFFDDFDFFDDFDDF\n",
      "@seq46647-928\n",
      "GACCTAGCGGTCATAAGTGGTACATTACGAGATTCGGAGTACCATAGATTCGCATGAATCCCTGTGGATACGAGAGTGTGAGATATATGTACGCCAATCCAGTGTGATACCCATGAGATTTAGGACCGATGATGGTTGACGACCAAGGATTGACCCGATGGATGCAGATTTGACCCCAGATAGAATAAATGCGATGAGATGATTTGGCCGATAGATAGATAGTAAGTAAATGCCACGGACTCGTCACGTG\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "head -10 hawkins_pooled_sequences.fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getMutations.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pysam needs python 3.6 downgrading from 3.8 now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coverage at base 0 = 980\n",
      "{}\n",
      "coverage at base 1 = 980\n",
      "{}\n",
      "coverage at base 2 = 980\n",
      "{}\n",
      "coverage at base 3 = 980\n",
      "{}\n",
      "coverage at base 4 = 980\n",
      "{}\n",
      "coverage at base 5 = 980\n",
      "{}\n",
      "coverage at base 6 = 980\n",
      "{}\n",
      "coverage at base 7 = 980\n",
      "{}\n",
      "coverage at base 8 = 980\n",
      "{}\n",
      "coverage at base 9 = 980\n",
      "{}\n",
      "coverage at base 10 = 980\n",
      "{}\n",
      "coverage at base 11 = 980\n",
      "{}\n",
      "coverage at base 12 = 980\n",
      "{}\n",
      "coverage at base 13 = 980\n",
      "{}\n",
      "coverage at base 14 = 980\n",
      "{}\n",
      "coverage at base 15 = 980\n",
      "{}\n",
      "coverage at base 16 = 980\n",
      "{}\n",
      "coverage at base 17 = 980\n",
      "{}\n",
      "coverage at base 18 = 980\n",
      "{}\n",
      "coverage at base 19 = 980\n",
      "{}\n",
      "coverage at base 20 = 980\n",
      "{}\n",
      "coverage at base 21 = 980\n",
      "{}\n",
      "coverage at base 22 = 980\n",
      "{}\n",
      "coverage at base 23 = 980\n",
      "{}\n",
      "coverage at base 24 = 980\n",
      "{}\n",
      "coverage at base 25 = 980\n",
      "{}\n",
      "coverage at base 26 = 980\n",
      "{}\n",
      "coverage at base 27 = 980\n",
      "{}\n",
      "coverage at base 28 = 980\n",
      "{}\n",
      "coverage at base 29 = 980\n",
      "{}\n",
      "coverage at base 30 = 980\n",
      "{}\n",
      "coverage at base 31 = 980\n",
      "{}\n",
      "coverage at base 32 = 980\n",
      "{}\n",
      "coverage at base 33 = 980\n",
      "{}\n",
      "coverage at base 34 = 980\n",
      "{}\n",
      "coverage at base 35 = 980\n",
      "{}\n",
      "coverage at base 36 = 980\n",
      "{}\n",
      "coverage at base 37 = 980\n",
      "{}\n",
      "coverage at base 38 = 980\n",
      "{}\n",
      "coverage at base 39 = 980\n",
      "{}\n",
      "coverage at base 40 = 980\n",
      "{}\n",
      "coverage at base 41 = 980\n",
      "{}\n",
      "coverage at base 42 = 980\n",
      "{}\n",
      "coverage at base 43 = 980\n",
      "{}\n",
      "coverage at base 44 = 980\n",
      "{}\n",
      "coverage at base 45 = 980\n",
      "{}\n",
      "coverage at base 46 = 980\n",
      "{}\n",
      "coverage at base 47 = 980\n",
      "{}\n",
      "coverage at base 48 = 980\n",
      "{}\n",
      "coverage at base 49 = 980\n",
      "{}\n",
      "coverage at base 50 = 980\n",
      "{}\n",
      "coverage at base 51 = 980\n",
      "{}\n",
      "coverage at base 52 = 980\n",
      "{}\n",
      "coverage at base 53 = 980\n",
      "{}\n",
      "coverage at base 54 = 980\n",
      "{}\n",
      "coverage at base 55 = 980\n",
      "{}\n",
      "coverage at base 56 = 980\n",
      "{}\n",
      "coverage at base 57 = 980\n",
      "{}\n",
      "coverage at base 58 = 980\n",
      "{}\n",
      "coverage at base 59 = 980\n",
      "{}\n",
      "coverage at base 60 = 980\n",
      "{}\n",
      "coverage at base 61 = 980\n",
      "{}\n",
      "coverage at base 62 = 980\n",
      "{}\n",
      "coverage at base 63 = 980\n",
      "{}\n",
      "coverage at base 64 = 980\n",
      "{}\n",
      "coverage at base 65 = 980\n",
      "{}\n",
      "coverage at base 66 = 980\n",
      "{}\n",
      "coverage at base 67 = 980\n",
      "{}\n",
      "coverage at base 68 = 980\n",
      "{}\n",
      "coverage at base 69 = 980\n",
      "{}\n",
      "coverage at base 70 = 980\n",
      "{}\n",
      "coverage at base 71 = 980\n",
      "{}\n",
      "coverage at base 72 = 980\n",
      "{}\n",
      "coverage at base 73 = 980\n",
      "{}\n",
      "coverage at base 74 = 980\n",
      "{}\n",
      "coverage at base 75 = 980\n",
      "{}\n",
      "coverage at base 76 = 980\n",
      "{}\n",
      "coverage at base 77 = 980\n",
      "{}\n",
      "coverage at base 78 = 980\n",
      "{}\n",
      "coverage at base 79 = 980\n",
      "{}\n",
      "coverage at base 80 = 980\n",
      "{}\n",
      "coverage at base 81 = 980\n",
      "{}\n",
      "coverage at base 82 = 980\n",
      "{}\n",
      "coverage at base 83 = 980\n",
      "{}\n",
      "coverage at base 84 = 980\n",
      "{}\n",
      "coverage at base 85 = 980\n",
      "{}\n",
      "coverage at base 86 = 980\n",
      "{}\n",
      "coverage at base 87 = 980\n",
      "{}\n",
      "coverage at base 88 = 480\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "import pysam\n",
    "\n",
    "#This is a slightly modified version from here: https://pysam.readthedocs.io/en/latest/api.html\n",
    "# What is a pileup? It is \"piling up\" all the reads that have mapped to a given position of your reference.\n",
    "# It is a useful way to see what reads have a mutation and what don't. \n",
    "\n",
    "def pileup():\n",
    "    #test file, replaced with the sorted.bam you are using. Make sure it is indexed! \n",
    "    #(Use samtools index yourbam.sorted.bam)\n",
    "    samfile = pysam.AlignmentFile(\"/home/rbif/week6/necessary_scripts/faker.sorted.bam\", \"rb\")\n",
    "\n",
    "    #Since our reference only has a single sequence, we're going to pile up ALL of the reads. \n",
    "    #Usually you would do it in a specific region (such as chromosome 1, position 1023 to 1050 for example)\n",
    "    for pileupcolumn in samfile.pileup():\n",
    "        print (\"coverage at base %s = %s\" % (pileupcolumn.pos, pileupcolumn.n))\n",
    "        #use a dictionary to count up the bases at each position\n",
    "        ntdict = {}\n",
    "        for pileupread in pileupcolumn.pileups:\n",
    "            if not pileupread.is_del and not pileupread.is_refskip:\n",
    "                # You can uncomment the below line to see what is happening in the pileup. \n",
    "                # print ('\\tbase in read %s = %s' % (pileupread.alignment.query_name, pileupread.alignment.query_sequence[pileupread.query_position]))\n",
    "                base = pileupread.alignment.query_sequence[pileupread.query_position]\n",
    "                \n",
    "                ########## ADD ADDITIONAL CODE HERE ############# \n",
    "                # Populate the ntdict with the counts of each base \n",
    "                # This dictionary will hold all of the base read counts per nucletoide per position.\n",
    "                # Use the dictionary to calculate the frequency of each site, and report it if if the frequency is NOT  100% / 0%. \n",
    "                #############################################\n",
    "        print (ntdict)\n",
    "    samfile.close()\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    pileup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parseFastq.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruggm/anaconda3/envs/learn/lib/python3.6/site-packages/ipykernel_launcher.py:23: DeprecationWarning: 'U' mode is deprecated\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "iter() returned non-iterator of type 'ParseFastQ'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e1f7546cb66a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m#A fastq read contains 4 lines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfastq_obj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfastqfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0;31m#This fastq_obj is a tuple that has length of 4 and corresponds to those 4 lines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m#This is the header\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: iter() returned non-iterator of type 'ParseFastQ'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "#Example use is \n",
    "# python parseFastq.py --fastq /home/rbif/week6/hawkins_pooled_sequences.fastq\n",
    "\n",
    "\n",
    "################################################\n",
    "# You can use this code and put it in your own script\n",
    "class ParseFastQ(object):\n",
    "    \"\"\"Returns a read-by-read fastQ parser analogous to file.readline()\"\"\"\n",
    "    def __init__(self,filePath,headerSymbols=['@','+']):\n",
    "        \"\"\"Returns a read-by-read fastQ parser analogous to file.readline().\n",
    "        Exmpl: parser.next()\n",
    "        -OR-\n",
    "        Its an iterator so you can do:\n",
    "        for rec in parser:\n",
    "            ... do something with rec ...\n",
    " \n",
    "        rec is tuple: (seqHeader,seqStr,qualHeader,qualStr)\n",
    "        \"\"\"\n",
    "        if filePath.endswith('.gz'):\n",
    "            self._file = gzip.open(filePath)\n",
    "        else:\n",
    "            self._file = open(filePath, 'rU')\n",
    "        self._currentLineNumber = 0\n",
    "        self._hdSyms = headerSymbols\n",
    "         \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "     \n",
    "    def next(self):\n",
    "        \"\"\"Reads in next element, parses, and does minimal verification.\n",
    "        Returns: tuple: (seqHeader,seqStr,qualHeader,qualStr)\"\"\"\n",
    "        # ++++ Get Next Four Lines ++++\n",
    "        elemList = []\n",
    "        for i in range(4):\n",
    "            line = self._file.readline()\n",
    "            self._currentLineNumber += 1 ## increment file position\n",
    "            if line:\n",
    "                elemList.append(line.strip('\\n'))\n",
    "            else: \n",
    "                elemList.append(None)\n",
    "         \n",
    "        # ++++ Check Lines For Expected Form ++++\n",
    "        trues = [bool(x) for x in elemList].count(True)\n",
    "        nones = elemList.count(None)\n",
    "        # -- Check for acceptable end of file --\n",
    "        if nones == 4:\n",
    "            raise StopIteration\n",
    "        # -- Make sure we got 4 full lines of data --\n",
    "        assert trues == 4,\\\n",
    "               \"** ERROR: It looks like I encountered a premature EOF or empty line.\\n\\\n",
    "               Please check FastQ file near line number %s (plus or minus ~4 lines) and try again**\" % (self._currentLineNumber)\n",
    "        # -- Make sure we are in the correct \"register\" --\n",
    "        assert elemList[0].startswith(self._hdSyms[0]),\\\n",
    "               \"** ERROR: The 1st line in fastq element does not start with '%s'.\\n\\\n",
    "               Please check FastQ file near line number %s (plus or minus ~4 lines) and try again**\" % (self._hdSyms[0],self._currentLineNumber) \n",
    "        assert elemList[2].startswith(self._hdSyms[1]),\\\n",
    "               \"** ERROR: The 3rd line in fastq element does not start with '%s'.\\n\\\n",
    "               Please check FastQ file near line number %s (plus or minus ~4 lines) and try again**\" % (self._hdSyms[1],self._currentLineNumber) \n",
    "        # -- Make sure the seq line and qual line have equal lengths --\n",
    "        assert len(elemList[1]) == len(elemList[3]), \"** ERROR: The length of Sequence data and Quality data of the last record aren't equal.\\n\\\n",
    "               Please check FastQ file near line number %s (plus or minus ~4 lines) and try again**\" % (self._currentLineNumber) \n",
    "         \n",
    "        # ++++ Return fatsQ data as tuple ++++\n",
    "        return tuple(elemList)\n",
    "##########################################################################\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-f\", \"--fastq\", required=True, help=\"Place fastq inside here\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    #This is an example of how to use the function in your own code\n",
    "    \n",
    "    fastqfile = ParseFastQ(args.fastq)\n",
    "\n",
    "    #A fastq read contains 4 lines\n",
    "    for fastq_obj in fastqfile:\n",
    "        #This fastq_obj is a tuple that has length of 4 and corresponds to those 4 lines\n",
    "        #This is the header\n",
    "        print(fastq_obj[0])\n",
    "        #This is the sequence\n",
    "        print(fastq_obj[1])\n",
    "        #This is the separator\n",
    "        print(fastq_obj[2])\n",
    "        #This is the quality score\n",
    "        print(fastq_obj[3])\n",
    "        \n",
    "        #Just an indicator showing the fastq \"blocks\"\n",
    "        print('*'*10 + '==='*10 + '*' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
