{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Environment-Review\" data-toc-modified-id=\"Environment-Review-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Environment Review</a></span><ul class=\"toc-item\"><li><span><a href=\"#Initial-Thoughts\" data-toc-modified-id=\"Initial-Thoughts-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Initial Thoughts</a></span></li><li><span><a href=\"#hawking_pooled_sequences.fastq\" data-toc-modified-id=\"hawking_pooled_sequences.fastq-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>hawking_pooled_sequences.fastq</a></span></li><li><span><a href=\"#getMutations.py\" data-toc-modified-id=\"getMutations.py-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>getMutations.py</a></span></li><li><span><a href=\"#parseFastq.py\" data-toc-modified-id=\"parseFastq.py-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>parseFastq.py</a></span></li></ul></li><li><span><a href=\"#Workflow-outline\" data-toc-modified-id=\"Workflow-outline-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Workflow outline</a></span><ul class=\"toc-item\"><li><span><a href=\"#Demultiplex-the-pooled-fastq-by-barcode\" data-toc-modified-id=\"Demultiplex-the-pooled-fastq-by-barcode-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Demultiplex the pooled fastq by barcode</a></span></li><li><span><a href=\"#Dictionary-Coding-Structure\" data-toc-modified-id=\"Dictionary-Coding-Structure-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Dictionary Coding Structure</a></span></li><li><span><a href=\"#Parsing-the-Parser\" data-toc-modified-id=\"Parsing-the-Parser-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Parsing the Parser</a></span></li><li><span><a href=\"#Merging-Dictionarys\" data-toc-modified-id=\"Merging-Dictionarys-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Merging Dictionarys</a></span></li><li><span><a href=\"#Quick-Sanity-Check\" data-toc-modified-id=\"Quick-Sanity-Check-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Quick Sanity Check</a></span></li><li><span><a href=\"#Undestanding-Pileup\" data-toc-modified-id=\"Undestanding-Pileup-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Undestanding Pileup</a></span></li></ul></li><li><span><a href=\"#Pipeline\" data-toc-modified-id=\"Pipeline-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Pipeline</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.3 (default, Mar 27 2019, 22:11:17) \\n[GCC 7.3.0]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assignment 3.pdf\n",
      "dgorgon_reference.fa\n",
      "getMutations.py\n",
      "harrington_clinical_data.txt\n",
      "hawkins_pooled_sequences.fastq\n",
      "necessary_scripts\n",
      "parseFastq.py\n",
      "__pycache__\n",
      "Week6_Lay_that_Pipe.ipynb\n",
      "\n",
      "faker.sorted.bam\n",
      "faker.sorted.bam.bai\n",
      "getMutations.py\n",
      "parseFastq.py\n",
      "\n",
      "----------------------\n",
      "Line Count of getMutations\n",
      "29 getMutations.py\n",
      "\n",
      "----------------------\n",
      "Line Count of parseFastq.py\n",
      "92 parseFastq.py\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "ls\n",
    "echo \"\"\n",
    "cd necessary_scripts/\n",
    "ls\n",
    "echo \"\"\n",
    "echo \"----------------------\"\n",
    "echo \"Line Count of getMutations\"\n",
    "wc -l getMutations.py\n",
    "echo \"\"\n",
    "echo \"----------------------\"\n",
    "echo \"Line Count of parseFastq.py\"\n",
    "wc -l parseFastq.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files are just too big, to look cat them out above, let's bring these files into jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hawking_pooled_sequences.fastq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@seq13534-419\n",
      "GCAGTAGCGGTCATAAGTGGTACATTACGAGATTCGGAGTACCATAGATTCGCATGAATCCCTGTGGATACGAGAGTGTGAGATATATGTACGCCAATCCAGTGTGATACCCATGAGATTTAGGACCGATGATGGTTGAGGACCAAGGATTGACCCGATGGATGCAGATTTGACCCCAGATAGAATAAATGCGATGAGATGATTTGGCCGATAGATAGATAGTGTCGTGAGGTGACGTCCGTCACTGGACGAA\n",
      "+\n",
      "IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIDIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIFFFFDFFDFFDDFDFDFFFFDDFFDDFDDFF\n",
      "@seq86249-867\n",
      "GGATTAGCGGTCATAAGTCGTACATTACGAGATTCGGAGTACCATAGATTCGCATGAATCCCTGTGGATACGAGAGTGTGAGATATATGTACGCCAATCCAGTGTGATACCCATGAGATTTAGGACCGATGATGGTTGAGGACCAAGGATTGACCCGATGGATGCAGATTTGACCCCAGATAGAATAAATGCGATGAGATGATTTGGCCGATAGATAGATAGAGGTCAGTATAACCTCTCAAAGCTTTATCTACGGATGGATCCGCGC\n",
      "+\n",
      "IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIDIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIDDFDDDDDDFFDFDDFDDDFDFFDDFFFFFFFFFDDFDFFDDFDDF\n",
      "@seq46647-928\n",
      "GACCTAGCGGTCATAAGTGGTACATTACGAGATTCGGAGTACCATAGATTCGCATGAATCCCTGTGGATACGAGAGTGTGAGATATATGTACGCCAATCCAGTGTGATACCCATGAGATTTAGGACCGATGATGGTTGACGACCAAGGATTGACCCGATGGATGCAGATTTGACCCCAGATAGAATAAATGCGATGAGATGATTTGGCCGATAGATAGATAGTAAGTAAATGCCACGGACTCGTCACGTG\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "head -10 hawkins_pooled_sequences.fastq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name\tColor\tBarcode\n",
      "Abbey\tBlack\tGCAGA\n",
      "Adelaide\tOrange\tCTTGA\n",
      "Alana\tBlack\tAGCTA\n",
      "Arabella\tBlack\tTTGAG\n",
      "Arianna\tOrange\tTCCGC\n",
      "Bobby\tBlack\tCCCAC\n",
      "Brayden\tBlack\tGCAGT\n",
      "Bridget\tOrange\tCGGTT\n",
      "Bruce\tOrange\tACGGA\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "head -10 harrington_clinical_data.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getMutations.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pysam needs python 3.6 downgrading from 3.8 now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting getMutations.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile getMutations.py\n",
    "import pysam\n",
    "\n",
    "#This is a slightly modified version from here: https://pysam.readthedocs.io/en/latest/api.html\n",
    "# What is a pileup? It is \"piling up\" all the reads that have mapped to a given position of your reference.\n",
    "# It is a useful way to see what reads have a mutation and what don't. \n",
    "\n",
    "def pileup():\n",
    "    #test file, replaced with the sorted.bam you are using. Make sure it is indexed! \n",
    "    #(Use samtools index yourbam.sorted.bam)\n",
    "    samfile = pysam.AlignmentFile(\"/home/rbif/week6/necessary_scripts/faker.sorted.bam\", \"rb\")\n",
    "\n",
    "    #Since our reference only has a single sequence, we're going to pile up ALL of the reads. \n",
    "    #Usually you would do it in a specific region (such as chromosome 1, position 1023 to 1050 for example)\n",
    "    for pileupcolumn in samfile.pileup():\n",
    "        print (\"coverage at base %s = %s\" % (pileupcolumn.pos, pileupcolumn.n))\n",
    "        #use a dictionary to count up the bases at each position\n",
    "        ntdict = {}\n",
    "        for pileupread in pileupcolumn.pileups:\n",
    "            if not pileupread.is_del and not pileupread.is_refskip:\n",
    "                # You can uncomment the below line to see what is happening in the pileup. \n",
    "                # print ('\\tbase in read %s = %s' % (pileupread.alignment.query_name, pileupread.alignment.query_sequence[pileupread.query_position]))\n",
    "                base = pileupread.alignment.query_sequence[pileupread.query_position]\n",
    "                \n",
    "                ########## ADD ADDITIONAL CODE HERE ############# \n",
    "                # Populate the ntdict with the counts of each base \n",
    "                # This dictionary will hold all of the base read counts per nucletoide per position.\n",
    "                # Use the dictionary to calculate the frequency of each site, and report it if if the frequency is NOT  100% / 0%. \n",
    "                #############################################\n",
    "        print (ntdict)\n",
    "    samfile.close()\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    pileup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parseFastq.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting parseFastq.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile parseFastq.py\n",
    "\n",
    "import argparse\n",
    "#Example use is \n",
    "# python parseFastq.py --fastq /home/rbif/week6/hawkins_pooled_sequences.fastq\n",
    "\n",
    "\n",
    "################################################\n",
    "# You can use this code and put it in your own script\n",
    "class ParseFastQ(object):\n",
    "    \"\"\"Returns a read-by-read fastQ parser analogous to file.readline()\"\"\"\n",
    "    def __init__(self,filePath,headerSymbols=['@','+']):\n",
    "        \"\"\"Returns a read-by-read fastQ parser analogous to file.readline().\n",
    "        Exmpl: parser.next()\n",
    "        -OR-\n",
    "        Its an iterator so you can do:\n",
    "        for rec in parser:\n",
    "            ... do something with rec ...\n",
    " \n",
    "        rec is tuple: (seqHeader,seqStr,qualHeader,qualStr)\n",
    "        \"\"\"\n",
    "        if filePath.endswith('.gz'):\n",
    "            self._file = gzip.open(filePath)\n",
    "        else:\n",
    "            self._file = open(filePath, 'rU')\n",
    "        self._currentLineNumber = 0\n",
    "        self._hdSyms = headerSymbols\n",
    "         \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    # I think this is supposed to be __next__(), so I'm changing it\n",
    "    def __next__(self):\n",
    "        \"\"\"Reads in next element, parses, and does minimal verification.\n",
    "        Returns: tuple: (seqHeader,seqStr,qualHeader,qualStr)\"\"\"\n",
    "        # ++++ Get Next Four Lines ++++\n",
    "        elemList = []\n",
    "        for i in range(4):\n",
    "            line = self._file.readline()\n",
    "            self._currentLineNumber += 1 ## increment file position\n",
    "            if line:\n",
    "                elemList.append(line.strip('\\n'))\n",
    "            else: \n",
    "                elemList.append(None)\n",
    "         \n",
    "        # ++++ Check Lines For Expected Form ++++\n",
    "        trues = [bool(x) for x in elemList].count(True)\n",
    "        nones = elemList.count(None)\n",
    "        # -- Check for acceptable end of file --\n",
    "        if nones == 4:\n",
    "            raise StopIteration\n",
    "        # -- Make sure we got 4 full lines of data --\n",
    "        assert trues == 4,\\\n",
    "               \"** ERROR: It looks like I encountered a premature EOF or empty line.\\n\\\n",
    "               Please check FastQ file near line number %s (plus or minus ~4 lines) and try again**\" % (self._currentLineNumber)\n",
    "        # -- Make sure we are in the correct \"register\" --\n",
    "        assert elemList[0].startswith(self._hdSyms[0]),\\\n",
    "               \"** ERROR: The 1st line in fastq element does not start with '%s'.\\n\\\n",
    "               Please check FastQ file near line number %s (plus or minus ~4 lines) and try again**\" % (self._hdSyms[0],self._currentLineNumber) \n",
    "        assert elemList[2].startswith(self._hdSyms[1]),\\\n",
    "               \"** ERROR: The 3rd line in fastq element does not start with '%s'.\\n\\\n",
    "               Please check FastQ file near line number %s (plus or minus ~4 lines) and try again**\" % (self._hdSyms[1],self._currentLineNumber) \n",
    "        # -- Make sure the seq line and qual line have equal lengths --\n",
    "        assert len(elemList[1]) == len(elemList[3]), \"** ERROR: The length of Sequence data and Quality data of the last record aren't equal.\\n\\\n",
    "               Please check FastQ file near line number %s (plus or minus ~4 lines) and try again**\" % (self._currentLineNumber) \n",
    "         \n",
    "        # ++++ Return fatsQ data as tuple ++++\n",
    "        return tuple(elemList)\n",
    "##########################################################################\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-f\", \"--fastq\", required=True, help=\"Place fastq inside here\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    #This is an example of how to use the function in your own code\n",
    "    \n",
    "    fastqfile = ParseFastQ(args.fastq)\n",
    "\n",
    "    #A fastq read contains 4 lines\n",
    "    for fastq_obj in fastqfile:\n",
    "        #This fastq_obj is a tuple that has length of 4 and corresponds to those 4 lines\n",
    "        #This is the header\n",
    "        print(fastq_obj[0])\n",
    "        #This is the sequence\n",
    "        print(fastq_obj[1])\n",
    "        #This is the separator\n",
    "        print(fastq_obj[2])\n",
    "        #This is the quality score\n",
    "        print(fastq_obj[3])\n",
    "        \n",
    "        #Just an indicator showing the fastq \"blocks\"\n",
    "        print('*'*10 + '==='*10 + '*' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "parseFastq.py:24: DeprecationWarning: 'U' mode is deprecated\n",
      "  self._file = open(filePath, 'rU')\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "python parseFastq.py --fastq /home/rbif/week6/hawkins_pooled_sequences.fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hmmm....this must be a really big output, let me see if I can implement this in a code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important note: We will not be able to use anything from BioPython for this assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demultiplex the pooled fastq by barcode\n",
    "\n",
    "Create a single python script that:\n",
    "1. Reads in the pooled fastq*\n",
    "2. Uses the barcode from clinical data to find matches\n",
    "3. Write to file\n",
    "4. Trims barcode from start\n",
    "5. Trims consequtive D or F from quality scores.\n",
    "6. Set in foder with {name}_trimmed.fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary Coding Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be using dictionarys to carry a lot of the heavy lifting inside this code. For example, I will be setting a coding_dict to make sure that everything is set in the right direction. Thus this will be more of a vinilla python programming exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGCGGTCATAAGTGGTACATTACGAGATTCGGAGTACCATAGATTCGCATGAATCCCTGTGGATACGAGAGTGTGAGATATATGTACGCCAATCCAGTGTGATACCCATGAGATTTAGGACCGATGATGGTTGAGGACCAAGGATTGACCCGATGGATGCAGATTTGACCCCAGATAGAATAAATGCGATGAGATGATTTGGCCGATAGATAGATAG\n"
     ]
    }
   ],
   "source": [
    "with open(\"dgorgon_reference.fa\") as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "dgorgon_reference = [x.strip() for x in content][1]\n",
    "\n",
    "print(dgorgon_reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parseFastq import ParseFastQ\n",
    "\n",
    "fastqfile = ParseFastQ(\"/home/rbif/week6/hawkins_pooled_sequences.fastq\")\n",
    "for item in fastqfile:\n",
    "    #It seems like item[2] are all \"+\", let's just check that really quick \n",
    "    if item[2] != \"+\": print(item[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Item's 0, 1, and 3 are important to the work, so we will keep them in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GCAGT'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fastqfile = ParseFastQ(\"/home/rbif/week6/hawkins_pooled_sequences.fastq\")\n",
    "fast_dict = {}\n",
    "\n",
    "for item in fastqfile:\n",
    "    fast_dict[item[0]] = {\"tag\":item[1][0:5], \n",
    "                          \"seq\":item[1][5:], \n",
    "                          \"quality\":item[3], \n",
    "                          \"name\":item[0]}\n",
    "\n",
    "fast_dict['@seq13534-419']['tag']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Dictionarys "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to get these dictionarys into one final format. It would probably be best if I put these by name, but names can repeat, but I assume they would not repeat the tag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parseFastq import ParseFastQ\n",
    "\n",
    "#First we are going to make a dictionary with all of the clinical entries\n",
    "with open(\"harrington_clinical_data.txt\") as clinical:\n",
    "    rows = (line.split('\\t') for line in clinical)\n",
    "    coding_dict = {row[1:][1].rstrip():{\"name\":row[0], \"color\":row[1]}  for row in rows}\n",
    "\n",
    "# Remove the first entry, we are not going to use a Pandas style index here\n",
    "coding_dict.pop('Barcode')\n",
    "\n",
    "fastqfile = ParseFastQ(\"hawkins_pooled_sequences.fastq\")\n",
    "for i in fastqfile:\n",
    "    # I don't think we need item 2 which is always a \"+\", so we will omit it\n",
    "    coding_dict[i[1][0:5]][i[0]] = {\"pretrimmed\":i[1][5:], \"prequality\":i[3]}\n",
    "    pretrim = coding_dict[i[1][0:5]][i[0]][\"prequality\"]\n",
    "\n",
    "    #Search for consecutive D or F \n",
    "    trim_value = re.search(r'[DF][DF]', pretrim).start()\n",
    "    coding_dict[i[1][0:5]][i[0]][\"trim_value\"] = trim_value\n",
    "    coding_dict[i[1][0:5]][i[0]][\"trimmed\"] = coding_dict[i[1][0:5]][i[0]][\"pretrimmed\"][:trim_value]\n",
    "    coding_dict[i[1][0:5]][i[0]][\"quality\"] = coding_dict[i[1][0:5]][i[0]][\"prequality\"][:trim_value]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abbey\n",
      "Adelaide\n",
      "Alana\n",
      "Arabella\n",
      "Arianna\n",
      "Bobby\n",
      "Brayden\n",
      "Bridget\n",
      "Bruce\n",
      "Byron\n",
      "Cali\n",
      "Camilla\n",
      "Case\n",
      "Cason\n",
      "Damien\n",
      "Dania\n",
      "Dereon\n",
      "Destiney\n",
      "Dominik\n",
      "Gabrielle\n",
      "Giancarlo\n",
      "Greta\n",
      "Israel\n",
      "Javion\n",
      "Jaylen\n",
      "Jazlyn\n",
      "Jordyn\n",
      "Jordyna\n",
      "Joshua\n",
      "Justus\n",
      "Kamden\n",
      "Karlee\n",
      "Kayla\n",
      "Keaton\n",
      "Kian\n",
      "Kirsten\n",
      "Libby\n",
      "Lorelei\n",
      "Mariela\n",
      "Martin\n",
      "Mckayla\n",
      "Mckenzie\n",
      "Natalia\n",
      "Santiago\n",
      "Sarai\n",
      "Serena\n",
      "Shelby\n",
      "Sienna\n",
      "Terry\n",
      "Tristian\n"
     ]
    }
   ],
   "source": [
    "for i in coding_dict:\n",
    "    print(coding_dict[i][\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r'[DF][DF]',coding_dict[\"GCAGA\"][\"@seq72638-854\"][\"quality\"]).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIFIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII\n",
      "DFFFFFDFDFDDFFDFDDFDFFFDFDFFDDFDDDDFFFF\n"
     ]
    }
   ],
   "source": [
    "print(coding_dict[\"GCAGA\"][\"@seq72638-854\"][\"quality\"][:222])\n",
    "print(coding_dict[\"GCAGA\"][\"@seq72638-854\"][\"quality\"][222:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to https://support.illumina.com/bulletins/2016/04/fastq-files-explained.html \n",
    "\n",
    "For each cluster that passes filter, a single sequence is written to the corresponding sample’s R1 FASTQ file, and, for a paired-end run, a single sequence is also written to the sample’s R2 FASTQ file. Each entry in a FASTQ files consists of 4 lines:\n",
    "\n",
    "1) A sequence identifier with information about the sequencing run and the cluster. The exact contents of this line vary by based on the BCL to FASTQ conversion software used. <br>\n",
    "2) The sequence (the base calls; A, C, T, G and N). <br>\n",
    "3) A separator, which is simply a plus (+) sign. <br>\n",
    "4) The base call quality scores. These are Phred +33 encoded, using ASCII characters to represent the numerical quality scores.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undestanding Pileup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "431\n",
      "{83: {'G': 50.11600928074246, 'C': 49.88399071925754, 'segments': 431}}\n"
     ]
    }
   ],
   "source": [
    "import pysam\n",
    "\n",
    "path = \"fastqs/Byron.sorted.bam\"\n",
    "\n",
    "samfile = pysam.AlignmentFile(path, \"rb\")\n",
    "i = 0\n",
    "\n",
    "count_dict = {}\n",
    "for item in samfile.pileup():\n",
    "    # Mapping qualities is not important for the problem\n",
    "    # print(item.get_mapping_qualities())\n",
    "    \n",
    "    # This is sort of useful returns alighed items\n",
    "    total_count = item.get_num_aligned()\n",
    "    \n",
    "    # This has the names of the aligned sequences, not useful here\n",
    "    # print(item.get_query_names())\n",
    "    \n",
    "    # Pileup column postions, not important here\n",
    "    # print(item.get_query_positions())\n",
    "    \n",
    "    # No use for quality score pileup\n",
    "    # print(item.get_query_qualities())\n",
    "    \n",
    "    # Jackpot\n",
    "    summary = item.get_query_sequences()\n",
    "    count_dict[item.pos] = {x:100*summary.count(x)/total_count for x in summary}\n",
    "\n",
    "mutation_dict = {}\n",
    "for i in count_dict.keys():\n",
    "    if len(count_dict[i].keys()) > 1:\n",
    "        mutation_dict[i] = count_dict[i]\n",
    "        mutation_dict[i][\"segments\"] = total_count\n",
    "#     print(item.n)\n",
    "print(item.nsegments)\n",
    "#     print(item.pileups)\n",
    "#     print(item.pos)\n",
    "#     print(item.reference_id)\n",
    "#     print(item.reference_name)\n",
    "#     print(item.reference_pos)\n",
    "#     print(item.set_min_base_quality)\n",
    "#     print(item.tid)\n",
    "print(mutation_dict)   \n",
    "samfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will try to make a function for every section of the homework to help read the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing Old Directory\n"
     ]
    }
   ],
   "source": [
    "#%%writefile pipeline.py\n",
    "from parseFastq import ParseFastQ\n",
    "import re\n",
    "import os\n",
    "import shutil\n",
    "import pysam\n",
    "import datetime\n",
    "\n",
    "class Main:\n",
    "    def __init__(self, har_clinic, hawkins_pooled):\n",
    "        \n",
    "        # Load reference\n",
    "        with open(\"dgorgon_reference.fa\") as f:\n",
    "            dgogron = f.readlines()\n",
    "        dgorgon_reference = [x.strip() for x in dgogron][1]\n",
    "        \n",
    "        # Create Report\n",
    "        report = open(\"report.txt\", \"w\")\n",
    "        now = datetime.datetime.now()\n",
    "        report.write(\"\\nAnalysis Run Started:\\n\" + now.strftime(\"%c\") + \"\\n---samples---\\n\")\n",
    "        report.close()\n",
    "        \n",
    "        coding_dict = self.trimmer(har_clinic, hawkins_pooled)\n",
    "        self.alignment()\n",
    "        \n",
    "        color_report = {}\n",
    "        for i in coding_dict:\n",
    "            file_path = \"fastqs/{}.sorted.bam\".format(coding_dict[i][\"name\"])\n",
    "            coding_dict[i][\"mutation\"] = self.pileup(file_path)\n",
    "            coding_mutation = coding_dict[i][\"mutation\"]\n",
    "            expected = dgorgon_reference[coding_mutation[\"position\"]]\n",
    "            \n",
    "            # This syntax is terrible, but I am running out of time\n",
    "            # for big big sets, it actually might be faster, however\n",
    "            mutation = coding_mutation[\"possible\"] - {expected}\n",
    "            mutation = list(mutation)[0]\n",
    "            \n",
    "            # Color Keeping: I think we should also have a count, but not asked for...\n",
    "            if coding_dict[i][\"color\"] not in color_report:\n",
    "                color_report[coding_dict[i][\"color\"]] = {\"expected\": expected,\n",
    "                                                         \"mutation\": mutation,\n",
    "                                                         \"position\": coding_mutation[\"position\"]}\n",
    "                \n",
    "            \n",
    "            with open(\"report.txt\", \"a\") as myfile:\n",
    "                report_text = \"Sample {0} had a {1} mold, \" \\\n",
    "                              \"{2} reads, and had {3}% of the reads at \" \\\n",
    "                              \"position {4} had \" \\\n",
    "                              \"the mutation {5}. \\n\".format(coding_dict[i][\"name\"],\n",
    "                                                            coding_dict[i][\"color\"],\n",
    "                                                            coding_mutation[\"reads\"],\n",
    "                                                            round(coding_mutation[mutation],2),\n",
    "                                                            coding_mutation[\"position\"],\n",
    "                                                            mutation)\n",
    "                \n",
    "                \n",
    "                myfile.write(report_text)\n",
    "        \n",
    "        with open(\"report.txt\", \"a\") as myfile:\n",
    "            myfile.write(\"\\n---mold-colors---\\n\")\n",
    "            \n",
    "        for color in color_report:\n",
    "            with open(\"report.txt\", \"a\") as myfile:\n",
    "                report_text = \"The {0} mold was caused by a mutation \" \\\n",
    "                              \"in position {1}. The wildtype base was {2} \" \\\n",
    "                              \"and the mutation was {3}. \\n\".format(color,\n",
    "                                                                    color_report[color][\"position\"],\n",
    "                                                                    color_report[color][\"expected\"],\n",
    "                                                                    color_report[color][\"mutation\"])\n",
    "            \n",
    "                myfile.write(report_text)\n",
    "\n",
    "        \n",
    "    # Part I: Trim and report\n",
    "    def trimmer(self, clinical_path, fast_path):\n",
    "        #First we are going to make a dictionary with all of the clinical entries\n",
    "        with open(clinical_path) as clinical:\n",
    "            rows = (line.split('\\t') for line in clinical)\n",
    "            coding_dict = {row[1:][1].rstrip():{\"name\":row[0], \"color\":row[1]}  for row in rows}\n",
    "\n",
    "        # Remove the first entry, we are not going to use a Pandas style index here\n",
    "        coding_dict.pop('Barcode')\n",
    "\n",
    "        fastqfile = ParseFastQ(fast_path)\n",
    "        for i in fastqfile:\n",
    "            # I don't think we need item 2 which is always a \"+\", so we will omit it\n",
    "\n",
    "            coding_dict[i[1][0:5]][i[0]] = {\"pretrimmed\":i[1][5:], \"prequality\":i[3]}\n",
    "            pretrim = coding_dict[i[1][0:5]][i[0]][\"prequality\"]\n",
    "\n",
    "            #Search for consecutive D or F \n",
    "            trim_value = re.search(r'[DF][DF]', pretrim).start()\n",
    "            coding_dict[i[1][0:5]][i[0]][\"trim_value\"] = trim_value\n",
    "            coding_dict[i[1][0:5]][i[0]][\"trimmed\"] = coding_dict[i[1][0:5]][i[0]][\"pretrimmed\"][:trim_value]\n",
    "            coding_dict[i[1][0:5]][i[0]][\"quality\"] = coding_dict[i[1][0:5]][i[0]][\"prequality\"][:trim_value]\n",
    "\n",
    "        # Make a new directory unless it exists\n",
    "        try:\n",
    "            os.makedirs(\"fastqs\")\n",
    "            print(\"Made New Directory\")\n",
    "        except:\n",
    "            print(\"Removing Old Directory\")\n",
    "            shutil.rmtree(\"fastqs\")\n",
    "            os.makedirs(\"fastqs\")\n",
    "\n",
    "        for listing in coding_dict:\n",
    "            # First name file\n",
    "            f= open(\"fastqs/{}_trimmed.fastq\".format(coding_dict[listing][\"name\"]),\"w+\")\n",
    "            for seq in coding_dict[listing]:\n",
    "                # I kept the name and color in the dictionary, we can't add that to fastq file\n",
    "                if seq not in {\"name\", \"color\"}:\n",
    "                    f.write(seq + \"\\n\")\n",
    "                    f.write(coding_dict[listing][seq][\"trimmed\"] + \"\\n\")\n",
    "                    f.write(\"+\" + \"\\n\")\n",
    "                    f.write(coding_dict[listing][seq][\"quality\"] + \"\\n\")\n",
    "            \n",
    "        # Part 1 completed\n",
    "        return coding_dict\n",
    "    \n",
    "    # Part 2&3 alignment\n",
    "    def alignment(self):\n",
    "        os.system(\"bwa index dgorgon_reference.fa\")\n",
    "        for name in os.listdir(\"fastqs\"):\n",
    "            split = name.split(\"_\")[0]\n",
    "            os.system(\"bwa mem dgorgon_reference.fa fastqs/{0}_trimmed.fastq > fastqs/{0}.sam\".format(split))\n",
    "            os.system(\"samtools view -bS fastqs/{0}.sam > fastqs/{0}.bam\".format(split))\n",
    "            os.system(\"samtools sort -m 100M -o fastqs/{0}.sorted.bam fastqs/{0}.bam\".format(split))\n",
    "            os.system(\"samtools index fastqs/{0}.sorted.bam\".format(split))\n",
    "            # Part 2&3 Completed\n",
    "    \n",
    "    # Part 4 \n",
    "    def pileup(self,path):\n",
    "        samfile = pysam.AlignmentFile(path, \"rb\")\n",
    "        count_dict = {}\n",
    "        for item in samfile.pileup():\n",
    "            total_count = item.get_num_aligned()\n",
    "            summary = item.get_query_sequences()\n",
    "            count_dict[item.pos] = {x:100*summary.count(x)/total_count for x in summary}\n",
    "\n",
    "        mutation_dict = {}\n",
    "        for i in count_dict.keys():\n",
    "            if len(count_dict[i].keys()) > 1:\n",
    "                mutation_dict = count_dict[i]\n",
    "                mutation_dict[\"possible\"] = set(count_dict[i].keys())\n",
    "                mutation_dict[\"reads\"] = total_count\n",
    "                mutation_dict[\"position\"] = i\n",
    "                \n",
    "        samfile.close()\n",
    "        return mutation_dict\n",
    "\n",
    "if __name__== \"__main__\":    \n",
    "    \n",
    "    ###\n",
    "    # Important: changes these TXTs to match your file structure\n",
    "    ###\n",
    "    Main(\"harrington_clinical_data.txt\", \"hawkins_pooled_sequences.fastq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{2}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{1,2} - {1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
