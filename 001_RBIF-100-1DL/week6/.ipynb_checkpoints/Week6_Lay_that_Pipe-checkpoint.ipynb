{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.3 (default, Mar 27 2019, 22:11:17) \\n[GCC 7.3.0]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assignment 3.pdf\n",
      "dgorgon_reference.fa\n",
      "getMutations.py\n",
      "harrington_clinical_data.txt\n",
      "hawkins_pooled_sequences.fastq\n",
      "necessary_scripts\n",
      "parseFastq.py\n",
      "__pycache__\n",
      "Week6_Lay_that_Pipe.ipynb\n",
      "\n",
      "faker.sorted.bam\n",
      "faker.sorted.bam.bai\n",
      "getMutations.py\n",
      "parseFastq.py\n",
      "\n",
      "----------------------\n",
      "Line Count of getMutations\n",
      "29 getMutations.py\n",
      "\n",
      "----------------------\n",
      "Line Count of parseFastq.py\n",
      "92 parseFastq.py\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "ls\n",
    "echo \"\"\n",
    "cd necessary_scripts/\n",
    "ls\n",
    "echo \"\"\n",
    "echo \"----------------------\"\n",
    "echo \"Line Count of getMutations\"\n",
    "wc -l getMutations.py\n",
    "echo \"\"\n",
    "echo \"----------------------\"\n",
    "echo \"Line Count of parseFastq.py\"\n",
    "wc -l parseFastq.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files are just too big, to look cat them out above, let's bring these files into jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hawking_pooled_sequences.fastq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@seq13534-419\n",
      "GCAGTAGCGGTCATAAGTGGTACATTACGAGATTCGGAGTACCATAGATTCGCATGAATCCCTGTGGATACGAGAGTGTGAGATATATGTACGCCAATCCAGTGTGATACCCATGAGATTTAGGACCGATGATGGTTGAGGACCAAGGATTGACCCGATGGATGCAGATTTGACCCCAGATAGAATAAATGCGATGAGATGATTTGGCCGATAGATAGATAGTGTCGTGAGGTGACGTCCGTCACTGGACGAA\n",
      "+\n",
      "IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIDIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIFFFFDFFDFFDDFDFDFFFFDDFFDDFDDFF\n",
      "@seq86249-867\n",
      "GGATTAGCGGTCATAAGTCGTACATTACGAGATTCGGAGTACCATAGATTCGCATGAATCCCTGTGGATACGAGAGTGTGAGATATATGTACGCCAATCCAGTGTGATACCCATGAGATTTAGGACCGATGATGGTTGAGGACCAAGGATTGACCCGATGGATGCAGATTTGACCCCAGATAGAATAAATGCGATGAGATGATTTGGCCGATAGATAGATAGAGGTCAGTATAACCTCTCAAAGCTTTATCTACGGATGGATCCGCGC\n",
      "+\n",
      "IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIDIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIDDFDDDDDDFFDFDDFDDDFDFFDDFFFFFFFFFDDFDFFDDFDDF\n",
      "@seq46647-928\n",
      "GACCTAGCGGTCATAAGTGGTACATTACGAGATTCGGAGTACCATAGATTCGCATGAATCCCTGTGGATACGAGAGTGTGAGATATATGTACGCCAATCCAGTGTGATACCCATGAGATTTAGGACCGATGATGGTTGACGACCAAGGATTGACCCGATGGATGCAGATTTGACCCCAGATAGAATAAATGCGATGAGATGATTTGGCCGATAGATAGATAGTAAGTAAATGCCACGGACTCGTCACGTG\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "head -10 hawkins_pooled_sequences.fastq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name\tColor\tBarcode\n",
      "Abbey\tBlack\tGCAGA\n",
      "Adelaide\tOrange\tCTTGA\n",
      "Alana\tBlack\tAGCTA\n",
      "Arabella\tBlack\tTTGAG\n",
      "Arianna\tOrange\tTCCGC\n",
      "Bobby\tBlack\tCCCAC\n",
      "Brayden\tBlack\tGCAGT\n",
      "Bridget\tOrange\tCGGTT\n",
      "Bruce\tOrange\tACGGA\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "head -10 harrington_clinical_data.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getMutations.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pysam needs python 3.6 downgrading from 3.8 now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting getMutations.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile getMutations.py\n",
    "import pysam\n",
    "\n",
    "#This is a slightly modified version from here: https://pysam.readthedocs.io/en/latest/api.html\n",
    "# What is a pileup? It is \"piling up\" all the reads that have mapped to a given position of your reference.\n",
    "# It is a useful way to see what reads have a mutation and what don't. \n",
    "\n",
    "def pileup():\n",
    "    #test file, replaced with the sorted.bam you are using. Make sure it is indexed! \n",
    "    #(Use samtools index yourbam.sorted.bam)\n",
    "    samfile = pysam.AlignmentFile(\"/home/rbif/week6/necessary_scripts/faker.sorted.bam\", \"rb\")\n",
    "\n",
    "    #Since our reference only has a single sequence, we're going to pile up ALL of the reads. \n",
    "    #Usually you would do it in a specific region (such as chromosome 1, position 1023 to 1050 for example)\n",
    "    for pileupcolumn in samfile.pileup():\n",
    "        print (\"coverage at base %s = %s\" % (pileupcolumn.pos, pileupcolumn.n))\n",
    "        #use a dictionary to count up the bases at each position\n",
    "        ntdict = {}\n",
    "        for pileupread in pileupcolumn.pileups:\n",
    "            if not pileupread.is_del and not pileupread.is_refskip:\n",
    "                # You can uncomment the below line to see what is happening in the pileup. \n",
    "                # print ('\\tbase in read %s = %s' % (pileupread.alignment.query_name, pileupread.alignment.query_sequence[pileupread.query_position]))\n",
    "                base = pileupread.alignment.query_sequence[pileupread.query_position]\n",
    "                \n",
    "                ########## ADD ADDITIONAL CODE HERE ############# \n",
    "                # Populate the ntdict with the counts of each base \n",
    "                # This dictionary will hold all of the base read counts per nucletoide per position.\n",
    "                # Use the dictionary to calculate the frequency of each site, and report it if if the frequency is NOT  100% / 0%. \n",
    "                #############################################\n",
    "        print (ntdict)\n",
    "    samfile.close()\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    pileup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parseFastq.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting parseFastq.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile parseFastq.py\n",
    "\n",
    "import argparse\n",
    "#Example use is \n",
    "# python parseFastq.py --fastq /home/rbif/week6/hawkins_pooled_sequences.fastq\n",
    "\n",
    "\n",
    "################################################\n",
    "# You can use this code and put it in your own script\n",
    "class ParseFastQ(object):\n",
    "    \"\"\"Returns a read-by-read fastQ parser analogous to file.readline()\"\"\"\n",
    "    def __init__(self,filePath,headerSymbols=['@','+']):\n",
    "        \"\"\"Returns a read-by-read fastQ parser analogous to file.readline().\n",
    "        Exmpl: parser.next()\n",
    "        -OR-\n",
    "        Its an iterator so you can do:\n",
    "        for rec in parser:\n",
    "            ... do something with rec ...\n",
    " \n",
    "        rec is tuple: (seqHeader,seqStr,qualHeader,qualStr)\n",
    "        \"\"\"\n",
    "        if filePath.endswith('.gz'):\n",
    "            self._file = gzip.open(filePath)\n",
    "        else:\n",
    "            self._file = open(filePath, 'rU')\n",
    "        self._currentLineNumber = 0\n",
    "        self._hdSyms = headerSymbols\n",
    "         \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    # I think this is supposed to be __next__(), so I'm changing it\n",
    "    def __next__(self):\n",
    "        \"\"\"Reads in next element, parses, and does minimal verification.\n",
    "        Returns: tuple: (seqHeader,seqStr,qualHeader,qualStr)\"\"\"\n",
    "        # ++++ Get Next Four Lines ++++\n",
    "        elemList = []\n",
    "        for i in range(4):\n",
    "            line = self._file.readline()\n",
    "            self._currentLineNumber += 1 ## increment file position\n",
    "            if line:\n",
    "                elemList.append(line.strip('\\n'))\n",
    "            else: \n",
    "                elemList.append(None)\n",
    "         \n",
    "        # ++++ Check Lines For Expected Form ++++\n",
    "        trues = [bool(x) for x in elemList].count(True)\n",
    "        nones = elemList.count(None)\n",
    "        # -- Check for acceptable end of file --\n",
    "        if nones == 4:\n",
    "            raise StopIteration\n",
    "        # -- Make sure we got 4 full lines of data --\n",
    "        assert trues == 4,\\\n",
    "               \"** ERROR: It looks like I encountered a premature EOF or empty line.\\n\\\n",
    "               Please check FastQ file near line number %s (plus or minus ~4 lines) and try again**\" % (self._currentLineNumber)\n",
    "        # -- Make sure we are in the correct \"register\" --\n",
    "        assert elemList[0].startswith(self._hdSyms[0]),\\\n",
    "               \"** ERROR: The 1st line in fastq element does not start with '%s'.\\n\\\n",
    "               Please check FastQ file near line number %s (plus or minus ~4 lines) and try again**\" % (self._hdSyms[0],self._currentLineNumber) \n",
    "        assert elemList[2].startswith(self._hdSyms[1]),\\\n",
    "               \"** ERROR: The 3rd line in fastq element does not start with '%s'.\\n\\\n",
    "               Please check FastQ file near line number %s (plus or minus ~4 lines) and try again**\" % (self._hdSyms[1],self._currentLineNumber) \n",
    "        # -- Make sure the seq line and qual line have equal lengths --\n",
    "        assert len(elemList[1]) == len(elemList[3]), \"** ERROR: The length of Sequence data and Quality data of the last record aren't equal.\\n\\\n",
    "               Please check FastQ file near line number %s (plus or minus ~4 lines) and try again**\" % (self._currentLineNumber) \n",
    "         \n",
    "        # ++++ Return fatsQ data as tuple ++++\n",
    "        return tuple(elemList)\n",
    "##########################################################################\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-f\", \"--fastq\", required=True, help=\"Place fastq inside here\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    #This is an example of how to use the function in your own code\n",
    "    \n",
    "    fastqfile = ParseFastQ(args.fastq)\n",
    "\n",
    "    #A fastq read contains 4 lines\n",
    "    for fastq_obj in fastqfile:\n",
    "        #This fastq_obj is a tuple that has length of 4 and corresponds to those 4 lines\n",
    "        #This is the header\n",
    "        print(fastq_obj[0])\n",
    "        #This is the sequence\n",
    "        print(fastq_obj[1])\n",
    "        #This is the separator\n",
    "        print(fastq_obj[2])\n",
    "        #This is the quality score\n",
    "        print(fastq_obj[3])\n",
    "        \n",
    "        #Just an indicator showing the fastq \"blocks\"\n",
    "        print('*'*10 + '==='*10 + '*' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "parseFastq.py:24: DeprecationWarning: 'U' mode is deprecated\n",
      "  self._file = open(filePath, 'rU')\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "python parseFastq.py --fastq /home/rbif/week6/hawkins_pooled_sequences.fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hmmm....this must be a really big output, let me see if I can implement this in a code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important note: We will not be able to use anything from BioPython for this assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demultiplex the pooled fastq by barcode\n",
    "\n",
    "Create a single python script that:\n",
    "1. Reads in the pooled fastq*\n",
    "2. Uses the barcode from clinical data to find matches\n",
    "3. Write to file\n",
    "4. Trims barcode from start\n",
    "5. Trims consequtive D or F from quality scores.\n",
    "6. Set in foder with {name}_trimmed.fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary Coding Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be using dictionarys to carry a lot of the heavy lifting inside this code. For example, I will be setting a coding_dict to make sure that everything is set in the right direction. Thus this will be more of a vinilla python programming exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Brayden', 'color': 'Black'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"/home/rbif/week6/harrington_clinical_data.txt\") as fin:\n",
    "    rows = (line.split('\\t') for line in fin )\n",
    "    coding_dict = {row[1:][1].rstrip():{\"name\":row[0], \"color\":row[1]}  for row in rows}\n",
    "    \n",
    "coding_dict[\"GCAGT\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parseFastq import ParseFastQ\n",
    "\n",
    "fastqfile = ParseFastQ(\"/home/rbif/week6/hawkins_pooled_sequences.fastq\")\n",
    "for item in fastqfile:\n",
    "    #It seems like item[2] are all \"+\", let's just check that really quick \n",
    "    if item[2] != \"+\": print(item[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Item's 0, 1, and 3 are important to the work, so we will keep them in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GCAGT'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fastqfile = ParseFastQ(\"/home/rbif/week6/hawkins_pooled_sequences.fastq\")\n",
    "fast_dict = {}\n",
    "\n",
    "for item in fastqfile:\n",
    "    fast_dict[item[0]] = {\"tag\":item[1][0:5], \n",
    "                          \"seq\":item[1][5:], \n",
    "                          \"quality\":item[3], \n",
    "                          \"name\":item[0]}\n",
    "\n",
    "fast_dict['@seq13534-419']['tag']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Dictionarys "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to get these dictionarys into one final format. It would probably be best if I put these by name, but names can repeat, but I assume they would not repeat the tag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parseFastq import ParseFastQ\n",
    "\n",
    "#First we are going to make a dictionary with all of the clinical entries\n",
    "with open(\"harrington_clinical_data.txt\") as clinical:\n",
    "    rows = (line.split('\\t') for line in clinical)\n",
    "    coding_dict = {row[1:][1].rstrip():{\"name\":row[0], \"color\":row[1]}  for row in rows}\n",
    "\n",
    "# Remove the first entry, we are not going to use a Pandas style index here\n",
    "coding_dict.pop('Barcode')\n",
    "\n",
    "fastqfile = ParseFastQ(\"hawkins_pooled_sequences.fastq\")\n",
    "for i in fastqfile:\n",
    "    # I don't think we need item 2 which is always a \"+\", so we will omit it\n",
    "    coding_dict[i[1][0:5]][i[0]] = {\"pretrimmed\":i[1][5:], \"prequality\":i[3]}\n",
    "    pretrim = coding_dict[i[1][0:5]][i[0]][\"prequality\"]\n",
    "\n",
    "    #Search for consecutive D or F \n",
    "    trim_value = re.search(r'[DF][DF]', pretrim).start()\n",
    "    coding_dict[i[1][0:5]][i[0]][\"trim_value\"] = trim_value\n",
    "    coding_dict[i[1][0:5]][i[0]][\"trimmed\"] = coding_dict[i[1][0:5]][i[0]][\"pretrimmed\"][:trim_value]\n",
    "    coding_dict[i[1][0:5]][i[0]][\"quality\"] = coding_dict[i[1][0:5]][i[0]][\"prequality\"][:trim_value]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pretrimmed': 'AGCGGTCATAAGTCGTACATTACGAGATTCGGAGTACCATAGATTCGCATGAATCCCTGTGGATACGAGAGTGTGAGATATATGTACGCCAATCCAGTGTGATACCCATGAGATTTAGGACCGATGATGGTTGAGGACCAAGGATTGACCCGATGGATGCAGATTTGACCCCAGATAGAATAAATGCGATGAGATGATTTGGCCGATAGATAGATAGTGCTCGCGGAAAGGCGCGTAGAGCTGATATGCACAAGTA',\n",
       " 'prequality': 'IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIFIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIDFFFFFDFDFDDFFDFDDFDFFFDFDFFDDFDDDDFFFF',\n",
       " 'trim_value': 222,\n",
       " 'trimmed': 'AGCGGTCATAAGTCGTACATTACGAGATTCGGAGTACCATAGATTCGCATGAATCCCTGTGGATACGAGAGTGTGAGATATATGTACGCCAATCCAGTGTGATACCCATGAGATTTAGGACCGATGATGGTTGAGGACCAAGGATTGACCCGATGGATGCAGATTTGACCCCAGATAGAATAAATGCGATGAGATGATTTGGCCGATAGATAGATAGTGCTC',\n",
       " 'quality': 'IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIFIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII'}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coding_dict[\"GCAGA\"][\"@seq72638-854\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r'[DF][DF]',coding_dict[\"GCAGA\"][\"@seq72638-854\"][\"quality\"]).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIFIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII\n",
      "DFFFFFDFDFDDFFDFDDFDFFFDFDFFDDFDDDDFFFF\n"
     ]
    }
   ],
   "source": [
    "print(coding_dict[\"GCAGA\"][\"@seq72638-854\"][\"quality\"][:222])\n",
    "print(coding_dict[\"GCAGA\"][\"@seq72638-854\"][\"quality\"][222:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to https://support.illumina.com/bulletins/2016/04/fastq-files-explained.html \n",
    "\n",
    "For each cluster that passes filter, a single sequence is written to the corresponding sample’s R1 FASTQ file, and, for a paired-end run, a single sequence is also written to the sample’s R2 FASTQ file. Each entry in a FASTQ files consists of 4 lines:\n",
    "\n",
    "1) A sequence identifier with information about the sequencing run and the cluster. The exact contents of this line vary by based on the BCL to FASTQ conversion software used. <br>\n",
    "2) The sequence (the base calls; A, C, T, G and N). <br>\n",
    "3) A separator, which is simply a plus (+) sign. <br>\n",
    "4) The base call quality scores. These are Phred +33 encoded, using ASCII characters to represent the numerical quality scores.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undestanding Pileup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will try to make a function for every section of the homework to help read the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing Old Directory\n",
      "coverage at base 0 = 566\n",
      "{}\n",
      "coverage at base 1 = 566\n",
      "{}\n",
      "coverage at base 2 = 566\n",
      "{}\n",
      "coverage at base 3 = 566\n",
      "{}\n",
      "coverage at base 4 = 566\n",
      "{}\n",
      "coverage at base 5 = 566\n",
      "{}\n",
      "coverage at base 6 = 566\n",
      "{}\n",
      "coverage at base 7 = 566\n",
      "{}\n",
      "coverage at base 8 = 566\n",
      "{}\n",
      "coverage at base 9 = 566\n",
      "{}\n",
      "coverage at base 10 = 566\n",
      "{}\n",
      "coverage at base 11 = 566\n",
      "{}\n",
      "coverage at base 12 = 566\n",
      "{}\n",
      "coverage at base 13 = 566\n",
      "{}\n",
      "coverage at base 14 = 566\n",
      "{}\n",
      "coverage at base 15 = 566\n",
      "{}\n",
      "coverage at base 16 = 566\n",
      "{}\n",
      "coverage at base 17 = 566\n",
      "{}\n",
      "coverage at base 18 = 566\n",
      "{}\n",
      "coverage at base 19 = 566\n",
      "{}\n",
      "coverage at base 20 = 566\n",
      "{}\n",
      "coverage at base 21 = 566\n",
      "{}\n",
      "coverage at base 22 = 566\n",
      "{}\n",
      "coverage at base 23 = 566\n",
      "{}\n",
      "coverage at base 24 = 566\n",
      "{}\n",
      "coverage at base 25 = 566\n",
      "{}\n",
      "coverage at base 26 = 566\n",
      "{}\n",
      "coverage at base 27 = 566\n",
      "{}\n",
      "coverage at base 28 = 566\n",
      "{}\n",
      "coverage at base 29 = 566\n",
      "{}\n",
      "coverage at base 30 = 566\n",
      "{}\n",
      "coverage at base 31 = 566\n",
      "{}\n",
      "coverage at base 32 = 566\n",
      "{}\n",
      "coverage at base 33 = 566\n",
      "{}\n",
      "coverage at base 34 = 566\n",
      "{}\n",
      "coverage at base 35 = 566\n",
      "{}\n",
      "coverage at base 36 = 566\n",
      "{}\n",
      "coverage at base 37 = 566\n",
      "{}\n",
      "coverage at base 38 = 566\n",
      "{}\n",
      "coverage at base 39 = 566\n",
      "{}\n",
      "coverage at base 40 = 566\n",
      "{}\n",
      "coverage at base 41 = 566\n",
      "{}\n",
      "coverage at base 42 = 566\n",
      "{}\n",
      "coverage at base 43 = 566\n",
      "{}\n",
      "coverage at base 44 = 566\n",
      "{}\n",
      "coverage at base 45 = 566\n",
      "{}\n",
      "coverage at base 46 = 566\n",
      "{}\n",
      "coverage at base 47 = 566\n",
      "{}\n",
      "coverage at base 48 = 566\n",
      "{}\n",
      "coverage at base 49 = 566\n",
      "{}\n",
      "coverage at base 50 = 566\n",
      "{}\n",
      "coverage at base 51 = 566\n",
      "{}\n",
      "coverage at base 52 = 566\n",
      "{}\n",
      "coverage at base 53 = 566\n",
      "{}\n",
      "coverage at base 54 = 566\n",
      "{}\n",
      "coverage at base 55 = 566\n",
      "{}\n",
      "coverage at base 56 = 566\n",
      "{}\n",
      "coverage at base 57 = 566\n",
      "{}\n",
      "coverage at base 58 = 566\n",
      "{}\n",
      "coverage at base 59 = 566\n",
      "{}\n",
      "coverage at base 60 = 566\n",
      "{}\n",
      "coverage at base 61 = 566\n",
      "{}\n",
      "coverage at base 62 = 566\n",
      "{}\n",
      "coverage at base 63 = 566\n",
      "{}\n",
      "coverage at base 64 = 566\n",
      "{}\n",
      "coverage at base 65 = 566\n",
      "{}\n",
      "coverage at base 66 = 566\n",
      "{}\n",
      "coverage at base 67 = 566\n",
      "{}\n",
      "coverage at base 68 = 566\n",
      "{}\n",
      "coverage at base 69 = 566\n",
      "{}\n",
      "coverage at base 70 = 566\n",
      "{}\n",
      "coverage at base 71 = 566\n",
      "{}\n",
      "coverage at base 72 = 566\n",
      "{}\n",
      "coverage at base 73 = 566\n",
      "{}\n",
      "coverage at base 74 = 566\n",
      "{}\n",
      "coverage at base 75 = 566\n",
      "{}\n",
      "coverage at base 76 = 566\n",
      "{}\n",
      "coverage at base 77 = 566\n",
      "{}\n",
      "coverage at base 78 = 566\n",
      "{}\n",
      "coverage at base 79 = 566\n",
      "{}\n",
      "coverage at base 80 = 566\n",
      "{}\n",
      "coverage at base 81 = 566\n",
      "{}\n",
      "coverage at base 82 = 566\n",
      "{}\n",
      "coverage at base 83 = 566\n",
      "{}\n",
      "coverage at base 84 = 566\n",
      "{}\n",
      "coverage at base 85 = 566\n",
      "{}\n",
      "coverage at base 86 = 566\n",
      "{}\n",
      "coverage at base 87 = 566\n",
      "{}\n",
      "coverage at base 88 = 566\n",
      "{}\n",
      "coverage at base 89 = 566\n",
      "{}\n",
      "coverage at base 90 = 566\n",
      "{}\n",
      "coverage at base 91 = 566\n",
      "{}\n",
      "coverage at base 92 = 566\n",
      "{}\n",
      "coverage at base 93 = 566\n",
      "{}\n",
      "coverage at base 94 = 566\n",
      "{}\n",
      "coverage at base 95 = 566\n",
      "{}\n",
      "coverage at base 96 = 566\n",
      "{}\n",
      "coverage at base 97 = 566\n",
      "{}\n",
      "coverage at base 98 = 566\n",
      "{}\n",
      "coverage at base 99 = 566\n",
      "{}\n",
      "coverage at base 100 = 566\n",
      "{}\n",
      "coverage at base 101 = 566\n",
      "{}\n",
      "coverage at base 102 = 566\n",
      "{}\n",
      "coverage at base 103 = 566\n",
      "{}\n",
      "coverage at base 104 = 566\n",
      "{}\n",
      "coverage at base 105 = 566\n",
      "{}\n",
      "coverage at base 106 = 566\n",
      "{}\n",
      "coverage at base 107 = 566\n",
      "{}\n",
      "coverage at base 108 = 566\n",
      "{}\n",
      "coverage at base 109 = 566\n",
      "{}\n",
      "coverage at base 110 = 566\n",
      "{}\n",
      "coverage at base 111 = 566\n",
      "{}\n",
      "coverage at base 112 = 566\n",
      "{}\n",
      "coverage at base 113 = 566\n",
      "{}\n",
      "coverage at base 114 = 566\n",
      "{}\n",
      "coverage at base 115 = 566\n",
      "{}\n",
      "coverage at base 116 = 566\n",
      "{}\n",
      "coverage at base 117 = 566\n",
      "{}\n",
      "coverage at base 118 = 566\n",
      "{}\n",
      "coverage at base 119 = 566\n",
      "{}\n",
      "coverage at base 120 = 566\n",
      "{}\n",
      "coverage at base 121 = 566\n",
      "{}\n",
      "coverage at base 122 = 566\n",
      "{}\n",
      "coverage at base 123 = 566\n",
      "{}\n",
      "coverage at base 124 = 566\n",
      "{}\n",
      "coverage at base 125 = 566\n",
      "{}\n",
      "coverage at base 126 = 566\n",
      "{}\n",
      "coverage at base 127 = 566\n",
      "{}\n",
      "coverage at base 128 = 566\n",
      "{}\n",
      "coverage at base 129 = 566\n",
      "{}\n",
      "coverage at base 130 = 566\n",
      "{}\n",
      "coverage at base 131 = 566\n",
      "{}\n",
      "coverage at base 132 = 566\n",
      "{}\n",
      "coverage at base 133 = 566\n",
      "{}\n",
      "coverage at base 134 = 566\n",
      "{}\n",
      "coverage at base 135 = 566\n",
      "{}\n",
      "coverage at base 136 = 566\n",
      "{}\n",
      "coverage at base 137 = 566\n",
      "{}\n",
      "coverage at base 138 = 566\n",
      "{}\n",
      "coverage at base 139 = 566\n",
      "{}\n",
      "coverage at base 140 = 566\n",
      "{}\n",
      "coverage at base 141 = 566\n",
      "{}\n",
      "coverage at base 142 = 566\n",
      "{}\n",
      "coverage at base 143 = 566\n",
      "{}\n",
      "coverage at base 144 = 566\n",
      "{}\n",
      "coverage at base 145 = 566\n",
      "{}\n",
      "coverage at base 146 = 566\n",
      "{}\n",
      "coverage at base 147 = 566\n",
      "{}\n",
      "coverage at base 148 = 566\n",
      "{}\n",
      "coverage at base 149 = 566\n",
      "{}\n",
      "coverage at base 150 = 566\n",
      "{}\n",
      "coverage at base 151 = 566\n",
      "{}\n",
      "coverage at base 152 = 566\n",
      "{}\n",
      "coverage at base 153 = 566\n",
      "{}\n",
      "coverage at base 154 = 566\n",
      "{}\n",
      "coverage at base 155 = 566\n",
      "{}\n",
      "coverage at base 156 = 566\n",
      "{}\n",
      "coverage at base 157 = 566\n",
      "{}\n",
      "coverage at base 158 = 566\n",
      "{}\n",
      "coverage at base 159 = 566\n",
      "{}\n",
      "coverage at base 160 = 566\n",
      "{}\n",
      "coverage at base 161 = 566\n",
      "{}\n",
      "coverage at base 162 = 566\n",
      "{}\n",
      "coverage at base 163 = 566\n",
      "{}\n",
      "coverage at base 164 = 566\n",
      "{}\n",
      "coverage at base 165 = 566\n",
      "{}\n",
      "coverage at base 166 = 566\n",
      "{}\n",
      "coverage at base 167 = 566\n",
      "{}\n",
      "coverage at base 168 = 566\n",
      "{}\n",
      "coverage at base 169 = 566\n",
      "{}\n",
      "coverage at base 170 = 566\n",
      "{}\n",
      "coverage at base 171 = 566\n",
      "{}\n",
      "coverage at base 172 = 566\n",
      "{}\n",
      "coverage at base 173 = 566\n",
      "{}\n",
      "coverage at base 174 = 566\n",
      "{}\n",
      "coverage at base 175 = 566\n",
      "{}\n",
      "coverage at base 176 = 566\n",
      "{}\n",
      "coverage at base 177 = 566\n",
      "{}\n",
      "coverage at base 178 = 566\n",
      "{}\n",
      "coverage at base 179 = 566\n",
      "{}\n",
      "coverage at base 180 = 566\n",
      "{}\n",
      "coverage at base 181 = 566\n",
      "{}\n",
      "coverage at base 182 = 566\n",
      "{}\n",
      "coverage at base 183 = 566\n",
      "{}\n",
      "coverage at base 184 = 566\n",
      "{}\n",
      "coverage at base 185 = 566\n",
      "{}\n",
      "coverage at base 186 = 566\n",
      "{}\n",
      "coverage at base 187 = 566\n",
      "{}\n",
      "coverage at base 188 = 566\n",
      "{}\n",
      "coverage at base 189 = 566\n",
      "{}\n",
      "coverage at base 190 = 566\n",
      "{}\n",
      "coverage at base 191 = 566\n",
      "{}\n",
      "coverage at base 192 = 566\n",
      "{}\n",
      "coverage at base 193 = 566\n",
      "{}\n",
      "coverage at base 194 = 566\n",
      "{}\n",
      "coverage at base 195 = 566\n",
      "{}\n",
      "coverage at base 196 = 566\n",
      "{}\n",
      "coverage at base 197 = 566\n",
      "{}\n",
      "coverage at base 198 = 566\n",
      "{}\n",
      "coverage at base 199 = 566\n",
      "{}\n",
      "coverage at base 200 = 566\n",
      "{}\n",
      "coverage at base 201 = 566\n",
      "{}\n",
      "coverage at base 202 = 566\n",
      "{}\n",
      "coverage at base 203 = 566\n",
      "{}\n",
      "coverage at base 204 = 566\n",
      "{}\n",
      "coverage at base 205 = 566\n",
      "{}\n",
      "coverage at base 206 = 566\n",
      "{}\n",
      "coverage at base 207 = 566\n",
      "{}\n",
      "coverage at base 208 = 566\n",
      "{}\n",
      "coverage at base 209 = 566\n",
      "{}\n",
      "coverage at base 210 = 566\n",
      "{}\n",
      "coverage at base 211 = 566\n",
      "{}\n",
      "coverage at base 212 = 566\n",
      "{}\n",
      "coverage at base 213 = 566\n",
      "{}\n",
      "coverage at base 214 = 566\n",
      "{}\n",
      "coverage at base 215 = 566\n",
      "{}\n",
      "coverage at base 216 = 566\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "#%%writefile pipeline.py\n",
    "from parseFastq import ParseFastQ\n",
    "import re\n",
    "import os\n",
    "import shutil\n",
    "import pysam\n",
    "\n",
    "class Main:\n",
    "    def __init__(self):\n",
    "        coding_dict = self.trimmer(\"harrington_clinical_data.txt\", \"hawkins_pooled_sequences.fastq\")\n",
    "        self.alignment()\n",
    "        self.pileup(\"fastqs/Brayden.sorted.bam\")\n",
    "\n",
    "    # Part I: Trim and report\n",
    "    def trimmer(self, clinical_path, fast_path):\n",
    "        #First we are going to make a dictionary with all of the clinical entries\n",
    "        with open(clinical_path) as clinical:\n",
    "            rows = (line.split('\\t') for line in clinical)\n",
    "            coding_dict = {row[1:][1].rstrip():{\"name\":row[0], \"color\":row[1]}  for row in rows}\n",
    "\n",
    "        # Remove the first entry, we are not going to use a Pandas style index here\n",
    "        coding_dict.pop('Barcode')\n",
    "\n",
    "        fastqfile = ParseFastQ(fast_path)\n",
    "        for i in fastqfile:\n",
    "            # I don't think we need item 2 which is always a \"+\", so we will omit it\n",
    "\n",
    "            coding_dict[i[1][0:5]][i[0]] = {\"pretrimmed\":i[1][5:], \"prequality\":i[3]}\n",
    "            pretrim = coding_dict[i[1][0:5]][i[0]][\"prequality\"]\n",
    "\n",
    "            #Search for consecutive D or F \n",
    "            trim_value = re.search(r'[DF][DF]', pretrim).start()\n",
    "            coding_dict[i[1][0:5]][i[0]][\"trim_value\"] = trim_value\n",
    "            coding_dict[i[1][0:5]][i[0]][\"trimmed\"] = coding_dict[i[1][0:5]][i[0]][\"pretrimmed\"][:trim_value]\n",
    "            coding_dict[i[1][0:5]][i[0]][\"quality\"] = coding_dict[i[1][0:5]][i[0]][\"prequality\"][:trim_value]\n",
    "\n",
    "        # Make a new directory unless it exists\n",
    "        try:\n",
    "            os.makedirs(\"fastqs\")\n",
    "            print(\"Made New Directory\")\n",
    "        except:\n",
    "            print(\"Removing Old Directory\")\n",
    "            shutil.rmtree(\"fastqs\")\n",
    "            os.makedirs(\"fastqs\")\n",
    "\n",
    "        for listing in coding_dict:\n",
    "            # First name file\n",
    "            f= open(\"fastqs/{}_trimmed.fastq\".format(coding_dict[listing][\"name\"]),\"w+\")\n",
    "            for seq in coding_dict[listing]:\n",
    "                # I kept the name and color in the dictionary, we can't add that to fastq file\n",
    "                if seq not in {\"name\", \"color\"}:\n",
    "                    f.write(seq + \"\\n\")\n",
    "                    f.write(coding_dict[listing][seq][\"trimmed\"] + \"\\n\")\n",
    "                    f.write(\"+\" + \"\\n\")\n",
    "                    f.write(coding_dict[listing][seq][\"quality\"] + \"\\n\")\n",
    "            \n",
    "        # Part 1 completed\n",
    "        return coding_dict\n",
    "    \n",
    "    # Part 2&3 alignment\n",
    "    def alignment(self):\n",
    "        os.system(\"bwa index dgorgon_reference.fa\")\n",
    "        for name in os.listdir(\"fastqs\"):\n",
    "            split = name.split(\"_\")[0]\n",
    "            os.system(\"bwa mem dgorgon_reference.fa fastqs/{0}_trimmed.fastq > fastqs/{0}.sam\".format(split))\n",
    "            os.system(\"samtools view -bS fastqs/{0}.sam > fastqs/{0}.bam\".format(split))\n",
    "            os.system(\"samtools sort -m 100M -o fastqs/{0}.sorted.bam fastqs/{0}.bam\".format(split))\n",
    "            os.system(\"samtools index fastqs/{0}.sorted.bam\".format(split))\n",
    "            # Part 2&3 Completed\n",
    "    \n",
    "    def pileup(self,path):\n",
    "        samfile = pysam.AlignmentFile(path, \"rb\")\n",
    "\n",
    "        #Since our reference only has a single sequence, we're going to pile up ALL of the reads. \n",
    "        #Usually you would do it in a specific region (such as chromosome 1, position 1023 to 1050 for example)\n",
    "        for pileupcolumn in samfile.pileup():\n",
    "            print (\"coverage at base %s = %s\" % (pileupcolumn.pos, pileupcolumn.n))\n",
    "            #use a dictionary to count up the bases at each position\n",
    "            ntdict = {}\n",
    "            for pileupread in pileupcolumn.pileups:\n",
    "                if not pileupread.is_del and not pileupread.is_refskip:\n",
    "                    # You can uncomment the below line to see what is happening in the pileup. \n",
    "                    # print('\\tbase in read %s = %s' % (pileupread.alignment.query_name, pileupread.alignment.query_sequence[pileupread.query_position]))\n",
    "                    base = pileupread.alignment.query_sequence[pileupread.query_position]\n",
    "\n",
    "                    ########## ADD ADDITIONAL CODE HERE ############# \n",
    "                    # Populate the ntdict with the counts of each base \n",
    "                    # This dictionary will hold all of the base read counts per nucletoide per position.\n",
    "                    # Use the dictionary to calculate the frequency of each site, and report it if if the frequency is NOT  100% / 0%. \n",
    "                    #############################################\n",
    "            print (ntdict)\n",
    "        samfile.close()\n",
    "        \n",
    "if __name__== \"__main__\":    \n",
    "    Main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
